### 1.- Gradient Descent for Multiple Linear Regression

#### a.- Please write down the gradient of <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/a092144e744b784c026531aeb02ebe05.svg?invert_in_darkmode" align=middle width=46.08219pt height=24.6576pt/>.

**Solution**
<p align="center"><img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/3b97788ea3acfd0a3b5f251be3ac14fb.svg?invert_in_darkmode" align=middle width=382.70925pt height=145.89366pt/></p>

Derivative with respect to <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/b40aa6b23d5447452eba56d9bda8960d.svg?invert_in_darkmode" align=middle width=11.40414pt height=22.83105pt/>:

<p align="center"><img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/8695802e37e3bf39ca2f225bddaf221c.svg?invert_in_darkmode" align=middle width=226.3107pt height=112.89366pt/></p>

#### b.- Please write down the step size you use in every iteration and explain why you use it.

**Solution**

<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/19f438a01644cc2d435fe2163d646892.svg?invert_in_darkmode" align=middle width=32.38356pt height=24.6576pt/> is twice differentiable and its Hessian
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/a679eb8d25635adc30688f9b6a952db6.svg?invert_in_darkmode" align=middle width=52.68219pt height=24.73119pt/> is
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/5044ebeabfb007e62eec3bb9713af741.svg?invert_in_darkmode" align=middle width=62.636145pt height=29.1819pt/>, which does not depend on
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/b40aa6b23d5447452eba56d9bda8960d.svg?invert_in_darkmode" align=middle width=11.40414pt height=22.83105pt/>. Because this cost function is convex we know that
its Hessian is positive semidefinite, so we have that (using the
definition of greatest curvature):

<p align="center"><img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/ea235affbbf932987d02785336e08946.svg?invert_in_darkmode" align=middle width=167.0724pt height=32.04366pt/></p>

Therefore, the smallest Lipschitz constant of <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/5eb3506cf8721a78598195593be1aa9b.svg?invert_in_darkmode" align=middle width=23.51613pt height=22.83105pt/> is the largest
eigenvalue of <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/9994b16b6628d5a6677ae72bf96200e8.svg?invert_in_darkmode" align=middle width=54.799305pt height=29.1819pt/>. We know that
matrix 2-norm induced by the euclidean vector norm is:

<p align="center"><img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/4d905543986a1be990461643f44ae310.svg?invert_in_darkmode" align=middle width=189.6411pt height=54.49752pt/></p>

Where <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6f6886e76c2b184507dd5c86bebd73c1.svg?invert_in_darkmode" align=middle width=35.838825pt height=22.83105pt/> is the largest number <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/fd8be73b54f5436a5cd2e73ba9b6bfa9.svg?invert_in_darkmode" align=middle width=9.58914pt height=22.83105pt/> such that
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/ca224eff0cba2fb696aa48272ec899da.svg?invert_in_darkmode" align=middle width=83.20488pt height=29.1819pt/> is singular.
i.e. <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6f6886e76c2b184507dd5c86bebd73c1.svg?invert_in_darkmode" align=middle width=35.838825pt height=22.83105pt/> is the largest eigenvalue of
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/c985ff35ac11fb296bbfb45ad6a587d4.svg?invert_in_darkmode" align=middle width=43.249305pt height=29.1819pt/>.

Since we want to take the biggest steps possible, we can compute the
Lipschitz constant as <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/2acc0b2282ce16ae0bdf055f8e1818d7.svg?invert_in_darkmode" align=middle width=83.263785pt height=24.73119pt/> and set a
fixed step size: 

<p align="center"><img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/11bf157475aa7824cccf265c28b857f9.svg?invert_in_darkmode" align=middle width=109.13298pt height=36.99366pt/></p>

#### c.- Please explain the rule of stopping your gradient descent algorithm.

**Solution**

In a descent method, as each new point is generated by the algorithm,
the corresponding value of the objective function decreases in value.
The gradient varies as the search proceeds, tending to zero as we
approach the minimizer.
The stopping rules that we will use:

-   Condition <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/2a1c77be09c69f643d4ec3c10c0fe6e8.svg?invert_in_darkmode" align=middle width=109.986855pt height=30.46032pt/>. However,
    this condition is not directly suitable as a practical stopping
    criterion because the numerical computation of the gradient will
    rarely be identically equal to zero. A practical criterion is to
    check if the norm $\|\nabla f(\boldsymbol{{\beta}}^{(k+1)})\|_2$ is
    less than a pre-specified threshold.

-   Also we will compute
    $\|f(\boldsymbol{{\beta}}^{(k+1)}) - f(\boldsymbol{{\beta}}^{(k)}\|$,
    and if the difference is less than some threshold, then we stop.

-   Finally, in some cases, to halt numerical algorithms a pre-specified
    number of iterations need to be specified, so if the above are not
    met the algorithm will stop after a finite number of iterations.

#### d.- Please draw a plot of <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/19f438a01644cc2d435fe2163d646892.svg?invert_in_darkmode" align=middle width=32.38356pt height=24.6576pt/> versus number of iterations to demonstrate the convergence of your algorithm.

**Solution**
After running 50 iterations we plot <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/ded7fd9e3caee9ddd7dfb775c93bbcf3.svg?invert_in_darkmode" align=middle width=31.781475pt height=24.6576pt/> versus number of iterations. We can see that the convergence of our algorithm is very fast:

<p align="center">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P1d_Plot.png" width= "60%" height= "60%">
</p>

#### e.- Please compare the result <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/e387e855178ab0e675692dd4a9da5edc.svg?invert_in_darkmode" align=middle width=16.033215pt height=22.83105pt/> returned by your algorithm with the true <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/8217ed3c32a785f0b5aad4055f432ad8.svg?invert_in_darkmode" align=middle width=10.16565pt height=22.83105pt/> by computing the mean squared error
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6cfccd7ca4e66b5ff16a69bf0a6a5263.svg?invert_in_darkmode" align=middle width=93.927405pt height=24.73119pt/>.

**Solution**
The mean squared error (MSE) is:

<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/4b51749b05c0df7572fa497462af56c5.svg?invert_in_darkmode" align=middle width=269.137605pt height=24.73119pt/>.

This is a very small MSE which was reached very fast. Our step size
choice was very good.

#### 2.- Stochastic Gradient Descent for Multiple Linear Regression

#### a.- Please draw a plot for the value of objective function <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/a333cbd1b4d2f846776de883fae2f5f1.svg?invert_in_darkmode" align=middle width=29.92407pt height=24.6576pt/> versus the number of iterations to demonstrate the convergence.

**Solution**
The one statistical unit gradient is:
<img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/d7bd8aa3647b4987baefdc9ca202acd1.svg?invert_in_darkmode" align=middle width=161.291955pt height=25.8819pt/>
Â 

The plot of objective function <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/a333cbd1b4d2f846776de883fae2f5f1.svg?invert_in_darkmode" align=middle width=29.92407pt height=24.6576pt/> versus the number of
iterations is:

<p align="center">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P2_Plot1.png" width= "60%" height= "60%">
</p>

We can see that with almost 1000 iterations the algorithm keeps looking
for an optimal value of the loss function. Convergence is not fast and
is not apparent that it has reached the minimum.

#### b.- Please draw three plots with <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/53d44ced554cb6b52a05021a96bb5d55.svg?invert_in_darkmode" align=middle width=96.40719pt height=22.83105pt/> for the value of objective function <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/a333cbd1b4d2f846776de883fae2f5f1.svg?invert_in_darkmode" align=middle width=29.92407pt height=24.6576pt/> versus the number of iterations to demonstrate the convergence.

**Solution**

We obtain the following three plots for <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/53d44ced554cb6b52a05021a96bb5d55.svg?invert_in_darkmode" align=middle width=96.40719pt height=22.83105pt/>:

<p align="center">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P2_Plot10.png" width= "60%" height= "60%">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P2_Plot25.png" width= "60%" height= "60%">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P2_Plot100.png" width= "60%" height= "60%">
</p>

We can see that as <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.0548555pt height=22.83105pt/> increases the algorithm converges faster. Almost
250 iterations for <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/7db7f7e73e60c422dd021d31b5e5d673.svg?invert_in_darkmode" align=middle width=42.878715pt height=22.83105pt/>, 100 iterations for <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/46e2ede37887084895560b57252748a3.svg?invert_in_darkmode" align=middle width=42.878715pt height=22.83105pt/> and about 20
iterations for <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/01c68b782233f7c21f731ee734f7c74e.svg?invert_in_darkmode" align=middle width=51.09786pt height=22.83105pt/>.

#### c.- Please compare the result <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/e387e855178ab0e675692dd4a9da5edc.svg?invert_in_darkmode" align=middle width=16.033215pt height=22.83105pt/> returned by your algorithm with the true <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/8217ed3c32a785f0b5aad4055f432ad8.svg?invert_in_darkmode" align=middle width=10.16565pt height=22.83105pt/> by computing the mean squared error <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6cfccd7ca4e66b5ff16a69bf0a6a5263.svg?invert_in_darkmode" align=middle width=93.927405pt height=24.73119pt/>.

**Solution**

The MSE for each <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.0548555pt height=22.83105pt/> are:

Algorithm & MSE

Stochastic Gradient Descent (b=1) : 0.0021607

Mini-batch Stochastic Gradient Descent (b=10) : 0.0000590

Mini-batch Stochastic Gradient Descent (b=25) : 0.0000656

Mini-batch Stochastic Gradient Descent (b=100) : 0.0000310

| Algorithm  |  MSE |
| ------------- | ------------- |
| Stochastic Gradient Descent (b=1)   | 0.0021607  |
| Mini-batch Stochastic Gradient Descent (b=10)   | 0.0000590  |
| Mini-batch Stochastic Gradient Descent (b=25)   | 0.0000656  |
| Mini-batch Stochastic Gradient Descent (b=100)   | 0.0000310  |


The larger the <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.0548555pt height=22.83105pt/> the smaller is the MSE. Is not only converging faster
but we get better results for larger batches.

#### 3.- Online Principal Component Analysis.


#### a.- Please draw a plot for <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6bf4a9ee6dec96d14ae4a65878b73df8.svg?invert_in_darkmode" align=middle width=71.549115pt height=24.6576pt/> versus number of iterations, where <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/c2a29561d89e139b3c7bffe51570c3ce.svg?invert_in_darkmode" align=middle width=16.41948pt height=14.15535pt/> is the output of Ojaâs algorithm at <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?invert_in_darkmode" align=middle width=5.663295pt height=21.68298pt/>th iteration and <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6c4adbc36120d62b98deef2a20d5d303.svg?invert_in_darkmode" align=middle width=8.55789pt height=14.15535pt/> is the
true top eigenvector of <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245pt height=22.46574pt/>.

**Solution**

Ojaâs algorithm with step size <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/9589854bc602818abe7e675f2b4fe3af.svg?invert_in_darkmode" align=middle width=75.24363pt height=24.6576pt/> constant for all
iteration seems to have a fast convergence but it has a jagged looking
plot, meaning that with each constant step it seems to get away and
closer at each iteration.

The final similarity measure <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/98a8470efd8ac57373ee8ba762c3dbac.svg?invert_in_darkmode" align=middle width=74.849115pt height=24.6576pt/> is 0.007513874.

<p align="center">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P3_Plota).png" width= "60%" height= "60%">
</p>

#### b.- Setting decreasing step sizes <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/c0e8998d58453399064b8005c52bdbce.svg?invert_in_darkmode" align=middle width=113.0778pt height=24.6576pt/> at <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?invert_in_darkmode" align=middle width=5.663295pt height=21.68298pt/>th iteration. Please draw a plot for <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6bf4a9ee6dec96d14ae4a65878b73df8.svg?invert_in_darkmode" align=middle width=71.549115pt height=24.6576pt/> versus number of iterations.

**Solution**Â 
When we use a variable step size <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/c0e8998d58453399064b8005c52bdbce.svg?invert_in_darkmode" align=middle width=113.0778pt height=24.6576pt/> at each <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?invert_in_darkmode" align=middle width=5.663295pt height=21.68298pt/>th
iteration the final similarity measure <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/98a8470efd8ac57373ee8ba762c3dbac.svg?invert_in_darkmode" align=middle width=74.849115pt height=24.6576pt/> is 0.0006637931.
Almost 10 times better accuracy than the previous result. Even though
the convergence is slower, we have a smoother plot that converges to a
more precise eigenvector <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/svgs/6c4adbc36120d62b98deef2a20d5d303.svg?invert_in_darkmode" align=middle width=8.55789pt height=14.15535pt/>.

<p align="center">
  <img src="https://rawgithub.com/jrecasens/Georgia-Tech/master/Stochastic-Gradient-Descent/img/P3_Plotb).png" width= "60%" height= "60%">
</p>
